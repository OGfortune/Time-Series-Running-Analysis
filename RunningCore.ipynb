{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c77ce0",
   "metadata": {},
   "source": [
    "# COMP47670 Assignment II Autumn 2023\n",
    "## Time Series Running Data\n",
    "\n",
    "## Objective\n",
    "The objective of this assignment is to identify good models for classifying time series data.  \n",
    "The data is from an accelerometer sensor and there are samples of fatigued and non-fatigued running. The data has been segmented into strides and the segments (samples) are labelled F (fatigued) and NF (not fatigued). The data for two subjects A and B are available in the files  `fatigueA.csv` and  `fatigueB.csv`. This dataset is extracted from a much larger dataset described [here](https://openreview.net/pdf?id=9c0lAonDNP).  \n",
    "At present, the best performing method for time-series classification is [Rocket](https://openreview.net/pdf?id=9c0lAonDNP). \n",
    "A rocket implementation is available in the [sktime tool kit](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.transformations.panel.rocket.Rocket.html). This sktime implementation can be used in this assignment.   \n",
    "Some code to get you started in available in the notebook `RunningCore`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ded7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost\n",
    "#pip install sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed655030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fatigue_df = pd.read_csv('fatigueA.csv', header = None) # sep = '\\s+')\n",
    "print(fatigue_df.shape)\n",
    "fatigue_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936131c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fatigue_df.iloc[4][1:].plot(label='Fatigued')\n",
    "fatigue_df.iloc[-5][1:].plot(figsize=(4,3), label = 'Not Fatigued')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),ncol=2)\n",
    "plt.ylabel('Accel Mag')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559bdcd9",
   "metadata": {},
   "source": [
    "## Task 1: Building Logistic Regression Classifier(with SDGClassifier) For Time Series Data\n",
    "Calculate the accuracy of a logistic regression classifier (`SGDClassifier`) on the raw time series data for subject A. \n",
    "\n",
    "**Objective:** The main objective of this task is to build a logistic regression which utilises the stochastic gradient descent algorithm which iteratively updates the weights and biases based on the gradient loss function of a during training.  The original dataset contains time series data derived from an accelerometer, segmented into strides and labelled as either fatigued (F) or not fatigued (NF).\n",
    "\n",
    "**Methodology:** Due to the sample size, I would be using the k-fold cross validation technique, to test the model's ability to accurately classify the classes correctly. `K-fold = 5`, to ensure as many possible subsets of the data are covered. Futhermore, cross-validations albeit being slower offer better accuracy estimates than hold out .\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an OrdinalEncoder\n",
    "\n",
    "ordinal_enc = OrdinalEncoder()\n",
    "\n",
    "#converting labels from F, NF, to binary 0,1\n",
    "\n",
    "fatigue_df[0] = ordinal_enc.fit_transform(fatigue_df[[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising X and y variables\n",
    "y = fatigue_df.pop(0).values\n",
    "X = fatigue_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ac36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creates an instance of SGDClassifier model\n",
    "sgd_clf_rocket = SGDClassifier(loss='log', random_state=42)\n",
    "\n",
    "# Performs cross-validation, k-fold=10\n",
    "scores = cross_val_score(sgd_clf_rocket, X, y, cv=5, scoring='accuracy') \n",
    "\n",
    "# Converting accuracy scores to percentage\n",
    "scores_percentage = [round(score * 100, 2) for score in scores]\n",
    "\n",
    "# Prints the accuracy for each fold\n",
    "print(\"Accuracy for each fold (%): \", scores_percentage)\n",
    "print(f\"Mean accuracy (%): , {round(scores.mean() * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd75d95",
   "metadata": {},
   "source": [
    "### Results\n",
    "The logistic regression had across the 10 k-folds had a mean acuuracy score of 77.6%, however, the rresults have a wide range with a minimum of 63.1% and a max of 95.4%. This suggests the model performs bettwer with certain subsets of the data compared to others. This could be due to several factors such as data variability between subsets, hyperparameters and \n",
    "\n",
    "#### Conclusion\n",
    "Task 1 successfully established a baseline for classifying time series data using a logistic regression approach. The moderate level of accuracy achieved suggests the potential for improved performance with more advanced methods, which will be explored in the following tasks of the project. This baseline serves as a foundation for further experimentation and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53115f94",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "The RunningCore Notebook contains code to convert the data to the `sktime` time-series format. Using this format assess the accuracy of the Rocket transformer coupled with an `SGDClassifier` classifier on the data for subject A. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff530507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X3d = X[:,np.newaxis,:] # time series algs require a 3D data array (sample, var, tick)\n",
    "X3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocket = Rocket(random_state=41)\n",
    "X3d_transformed = rocket.fit_transform(X3d)\n",
    "\n",
    "# Create an instance of SGDClassifier\n",
    "sgd_clf_rocket = SGDClassifier(loss='log', random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(sgd_clf_rocket, X3d_transformed, y, cv=5, scoring='accuracy')  # cv=5 for 5-fold cross-validation\n",
    "\n",
    "scores_percentage = [round(score * 100, 2) for score in scores]\n",
    "\n",
    "# Prints the accuracy for each fold\n",
    "print(\"Accuracy for each fold (%): \", scores_percentage)\n",
    "print(f\"Mean accuracy (%): , {round(scores.mean() * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07745a",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The mean accuracy score for the ROCKET transformed regression is 81.72% (range: 77.65%-94.05%). Applying the ROCKET improved the overall models accuracy from 78.64% in the baseline model, to 81.72%.\n",
    "\n",
    "### Conclusion\n",
    "The major increase in overall accuracy could be attributable to ROCKET's ability to efficiently capture complex patterns in time series data through its convolutional kernels. Unlike the basic logistic regression model, which directly handled raw time series, this approach transformed the data into a more informative feature space, thereby facilitating more accurate classifications.\n",
    "\n",
    "Overall, this approach and its finding sets a new benchmark for subsequent tasks as I analyse more complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454db8d4",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "**Objective:** The main objective of this task is to try and improve our accuracy score by employing alternative models, and performing data normalisation if neccessary.\n",
    "\n",
    "#### **Methodology:** Methods used include the following:\n",
    "\n",
    "1. **Normalisation:** I have employed the use of various normalisation techniques, these includes\n",
    "    * **No Normalisation:** In this method, the data would be left as is. that is in its original state\n",
    "    * **StandardScaler():** This normalise the data by subtracting the mean and dividing by variance. Hence features would have a mean of 0 and variance of 1\n",
    "    * **MinMaxScaler():** This normalises the data by scaling features to a particular range, usually 0-1\n",
    "\n",
    "2. **Classifiers:**\n",
    "    * SDGClassifier\n",
    "    * RandomForest Classifier\n",
    "    * Support Vector Machine (SVM)\n",
    "    * KNN\n",
    "    * Decision Tree\n",
    "    * XGBoost\n",
    "\n",
    "We would be calculating the accuracy of each model based on the normalisation techniques, with evaluation done useing cross validation `K-fold=5`, and be presenting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisation_techniques = ['NoNormalisation', StandardScaler(), MinMaxScaler(), Normalizer()]\n",
    "\n",
    "classifiers = {\n",
    "    'SGDClassifier': SGDClassifier(loss='log', random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'kNN': KNeighborsClassifier(),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "for normalisation in normalisation_techniques:\n",
    "    if normalisation == 'NoNormalisation':\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            cv_scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "            cv_scores_percentage = [round(score * 100, 2) for score in cv_scores]\n",
    "            print(f\"Cross-Validation without normalization and {clf_name}\")\n",
    "            print(\"CV Scores: \", cv_scores_percentage)\n",
    "            print(f\"Average CV Score:  {round(cv_scores.mean() * 100, 2)}%\")\n",
    "            print(\"--------------------------------------------\\n\")\n",
    "\n",
    "    else:\n",
    "        X_norm = normalisation.fit_transform(X)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            cv_scores = cross_val_score(clf, X_norm, y, cv=5, scoring='accuracy')\n",
    "            cv_scores_percentage = [round(score * 100, 2) for score in cv_scores]\n",
    "            print(f\"Cross-Validation with {normalisation} and {clf_name}\")\n",
    "            print(\"CV Scores: \", cv_scores_percentage)\n",
    "            print(f\"Average CV Score:  {round(cv_scores.mean() * 100, 2)}%\")\n",
    "            print(\"--------------------------------------------\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e1251",
   "metadata": {},
   "source": [
    "| Normalization Method     | Model          | Average CV Score (%) | Min CV Score (%) | Max CV Score (%) |\n",
    "|--------------------------|----------------|----------------------|------------------|------------------|\n",
    "| No Normalization         | SGDClassifier  | 78.64                | 63.10            | 95.24            |\n",
    "| No Normalization         | RandomForest   | 84.37                | 65.88            | 98.81            |\n",
    "| No Normalization         | SVM            | 80.54                | 70.24            | 94.05            |\n",
    "| No Normalization         | kNN            | 85.78                | 70.59            | 95.24            |\n",
    "| No Normalization         | DecisionTree   | 79.61                | 65.88            | 88.1             |\n",
    "| No Normalization         | XGBoost        | 86.74                | 70.59            | 98.81            |\n",
    "| StandardScaler()         | SGDClassifier  | 76.04                | 62.35            | 90.48            |\n",
    "| StandardScaler()         | RandomForest   | 84.37                | 65.88            | 98.81            |\n",
    "| StandardScaler()         | SVM            | 87.91                | 76.47            | 97.62            |\n",
    "| StandardScaler()         | kNN            | 85.07                | 71.76            | 95.24            |\n",
    "| StandardScaler()         | DecisionTree   | 79.37                | 65.88            | 88.1             |\n",
    "| StandardScaler()         | XGBoost        | 86.74                | 70.59            | 98.81            |\n",
    "| MinMaxScaler()           | SGDClassifier  | 81.48                | 77.38            | 86.9             |\n",
    "| MinMaxScaler()           | RandomForest   | 84.37                | 65.88            | 98.81            |\n",
    "| MinMaxScaler()           | SVM            | 84.83                | 77.38            | 94.05            |\n",
    "| MinMaxScaler()           | kNN            | 86.03                | 77.38            | 95.24            |\n",
    "| MinMaxScaler()           | DecisionTree   | 79.61                | 77.38            | 88.1             |\n",
    "| MinMaxScaler()           | XGBoost        | 86.74                | 70.59            | 98.81            |\n",
    "| Normalizer()             | SGDClassifier  | 80.04                | 65.48            | 89.29            |\n",
    "| Normalizer()             | RandomForest   | 85.07                | 70.59            | 97.62            |\n",
    "| Normalizer()             | SVM            | 81.03                | 69.41            | 94.05            |\n",
    "| Normalizer()             | kNN            | 83.41                | 65.88            | 95.24            |\n",
    "| Normalizer()             | DecisionTree   | 75.58                | 57.65            | 88.1             |\n",
    "| Normalizer()             | XGBoost        | 87.92                | 75.29            | 98.81            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d415903b",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "**No Normalization:**\n",
    "* The best performing model in the non-normalised datatset is teh XGBoost classifier with an accuracy of 86.74%(min:70.59%, max:98.81%). \n",
    "\n",
    "* The least performing model was the SGDClassifier with an accuracy score of 78.64% (min: 63.10%, max:95.24%)\n",
    "\n",
    "\n",
    "**StandardScaler() Normalization:**\n",
    "\n",
    "* The best performing model with StandardScaler() normalization is the SVM classifier with an accuracy of 87.91% (min: 76.47%, max: 97.62%).\n",
    "\n",
    "* The least performing model in this dataset with StandardScaler() normalization is the SGDClassifier with an accuracy score of 76.04% (min: 62.35%, max: 90.48%).\n",
    "\n",
    "\n",
    "**MinMaxScaler() Normalization:**\n",
    "\n",
    "* The best performing model with MinMaxScaler() normalization is the XGBoost classifier with an accuracy of 86.74% (min: 70.59%, max: 98.81%).\n",
    "\n",
    "* The least performing model in this dataset with MinMaxScaler() normalization is the SGDClassifier with an accuracy score of 81.48% (min: 77.38%, max: 85.71%).\n",
    "\n",
    "**Normalizer():**\n",
    "\n",
    "* The best performing model with Normalizer() normalization is the XGBoost classifier with an accuracy of 87.92%% (min: 75.29%, max: 98.81%).\n",
    "\n",
    "* The least performing model in this dataset with Normalizer() normalization is the DecisionTree with an accuracy score of 75.58% (min: 57.65%, max: 88.1%).\n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "----------------------------------------\n",
    "Overall, data normalisation had varying impact, although miniscule, on the overall accuracy of each model. By far across all four techniques employed, the best performing model was the SVM on the StandardScaler() input data with a score of 86.74% (min: 70.59%, max: 98.81%), while the DecisionTree on the Normalizer() data was the least performing in all 4 recording its lowest score 75.58% (min: 57.65%, max: 88.1%)) in the StandardScalar technique. The models also performed differently on different subset of the data reflecting howvariations in the data might affect results. Model's performance is also dependent on their algorithm's strength and weaknesses, some of the models such as SVM,KNN, Random forrest are not particularly sensitive to sacaling or normalisation as they are very robust, while others like SGDClassifier my be in comparison to the rest. Further hyperparameter tuning or having a larger dataset could help improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f6825",
   "metadata": {},
   "source": [
    "##  Task 4\n",
    "#### Objective\n",
    "In this task, the main focus is to optimise the time series classification by experimenting with different kernel sizes (or numbers), with the main aim of determining what impact the kernel size would have on the accuracy of various classifiers.\n",
    "\n",
    "#### Methodology\n",
    "As with the previous ROCKET task, the input would still be the previous 3d format created (i.e `X3d`). I would be using 4 different kernel numbers (5000, 10000, 15000, and 20000), by default the kernel number in sklearn is 10000. We would observe the accuracy of different classifiers accross these kernel sizes. the classifiers to be used include\n",
    "* SDGClassifier\n",
    "* RandomForest Classifier\n",
    "* Support Vector Machine (SVM)\n",
    "* KNN\n",
    "* Decision Tree\n",
    "* XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_options = [5000, 10000, 20000, 50000]\n",
    "\n",
    "for kernel in kernel_options:\n",
    "    rocket = Rocket(num_kernels=kernel, random_state=42)\n",
    "    X3d_transformed = rocket.fit_transform(X3d)\n",
    "     \n",
    "\n",
    "    for classifer_name, classifier in classifiers.items():\n",
    "        \n",
    "        classifier.fit(X3d_transformed, y)\n",
    "\n",
    "        \n",
    "        cv_scores = cross_val_score(classifier, X3d_transformed, y, cv=5, scoring='accuracy')\n",
    "        cv_scores_percentage = [round(score * 100, 2) for score in cv_scores]\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Cross-Validation with {kernel} kernels and {classifer_name}\")\n",
    "        print(\"CV Scores: \", cv_scores_percentage)\n",
    "        print(f\"Average CV Score:  {round(cv_scores.mean() * 100, 2)}%\")\n",
    "        print(\"----------------------------------------------------\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92619deb",
   "metadata": {},
   "source": [
    "| Kernels | Model          | Average CV Score (%) | Min CV Score (%) | Max CV Score (%) |\n",
    "|---------|----------------|----------------------|------------------|------------------|\n",
    "| 5000    | SGDClassifier  | 82.21                | 70.59            | 94.05            |\n",
    "| 5000    | RandomForest   | 93.13                | 84.71            | 97.62            |\n",
    "| 5000    | SVM            | 87.69                | 71.76            | 92.86            |\n",
    "| 5000    | kNN            | 87.93                | 69.41            | 96.43            |\n",
    "| 5000    | DecisionTree   | 82.45                | 72.94            | 91.67            |\n",
    "| 5000    | XGBoost        | 92.9                 | 82.35            | 96.43            |\n",
    "| 10000   | SGDClassifier  | 82.68                | 72.94            | 94.05            |\n",
    "| 10000   | RandomForest   | 92.18                | 82.35            | 97.62            |\n",
    "| 10000   | SVM            | 86.26                | 71.76            | 92.86            |\n",
    "| 10000   | kNN            | 87.69                | 68.24            | 97.62            |\n",
    "| 10000   | DecisionTree   | 85.98                | 79.76            | 90.48            |\n",
    "| 10000   | XGBoost        | 95.03                | 89.41            | 98.81            |\n",
    "| 20000   | SGDClassifier  | 81.06                | 52.94            | 96.43            |\n",
    "| 20000   | RandomForest   | 93.37                | 84.71            | 97.62            |\n",
    "| 20000   | SVM            | 86.02                | 71.76            | 92.86            |\n",
    "| 20000   | kNN            | 87.93                | 68.24            | 96.43            |\n",
    "| 20000   | DecisionTree   | 84.59                | 70.59            | 90.48            |\n",
    "| 20000   | XGBoost        | 95.26                | 90.59            | 100.0            |\n",
    "| 50000   | SGDClassifier  | 91.7                 | 84.52            | 96.43            |\n",
    "| 50000   | RandomForest   | 93.37                | 83.53            | 97.62            |\n",
    "| 50000   | SVM            | 86.97                | 71.76            | 92.86            |\n",
    "| 50000   | kNN            | 88.17                | 69.41            | 97.62            |\n",
    "| 50000   | DecisionTree   | 82.91                | 76.47            | 89.29            |\n",
    "| 50000   | XGBoost        | 92.19                | 80.0             | 100.0            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c17f9",
   "metadata": {},
   "source": [
    "#### Results\n",
    "-------------------------\n",
    "**Best Performing Models By Kernel Size:**\n",
    "* ROCKET = 5000 kernels\n",
    "    * Best Performing Model: The RandomForest was the best performing model with an average accuracy of 93.13% (Min: 84.71%, Max: 97.62%)\n",
    "    * Worst Performing Model: The SDGClassifier was the least performing model with an average accuracy score of 82.21% (Min: 70.95%, Max: 94.05%)\n",
    "\n",
    "* ROCKET = 10000 Kernels:\n",
    "    * Best Performing Model: The XGBoost was the best performing model with an average accuracy of 95.03% (Min: 89.41%, Max: 96.43%)\n",
    "    * Worst Performing Model: The SDGClassifier was the least performing model with an average accuracy score of 82.68% (Min: 72.62%, Max: 94.05%)\n",
    "\n",
    "\n",
    "* ROCKET = 20000 Kernels:\n",
    "    * Best Performing Model: The XGBoost was the best performing model with an average accuracy of 95.26% (Min: 90.59%, Max: 100.0%)\n",
    "    * Worst Performing Model: The SDGClassifier was the least performing model with an average accuracy score of 81.06% (Min: 52.94%, Max: 89.29%)\n",
    "\n",
    "* ROCKET = 50000 Kernels:\n",
    "    * Best Performing Model: The XGBoost was the best performing model with an average accuracy of 93.37% (Min: 83.53%, Max: 97.62%)\n",
    "    * Worst Performing Model: The DecisionTree was the least performing model with an average accuracy score of 82.91% (Min: 76.67%, Max: 89.29%)\n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "------------------\n",
    "Overall, both the XGBoost and RandomForest outperformed the other models in terms of overall accuracy, maintaining or improving performance highlighting their robustness in handling kernel complexities and scalability, with XGBoost recording the best accuracy (95.03% (Min: 89.41%, Max: 96.43%)) of all models when kernel = 5000. In contrast, both the DecisionTree models and the SDGClassifier seem to struggle at different kernel sizes. Also, SVM,  and kNN performance varied moderately across different kernel sizes but did not reach the high consistency of XGBoost or RandomForest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c22ae4",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "fatigue_df_b = pd.read_csv('fatigueB.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e088533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an OrdinalEncoder\n",
    "\n",
    "ordinal_enc = OrdinalEncoder()\n",
    "\n",
    "#converting labels from F, NF, to binary 0,1\n",
    "\n",
    "fatigue_df_b[0] = ordinal_enc.fit_transform(fatigue_df_b[[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea79627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising X and y variables\n",
    "y_b = fatigue_df_b.pop(0).values\n",
    "X_b = fatigue_df_b.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e082da18",
   "metadata": {},
   "source": [
    "### a. applying methodologies such as normlalisation used in Task 3 on Subject B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78552d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisation_techniques = ['NoNormalisation', StandardScaler(), MinMaxScaler(), Normalizer()]\n",
    "\n",
    "classifiers = {\n",
    "    'SGDClassifier': SGDClassifier(loss='log', random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'kNN': KNeighborsClassifier(),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "for normalisation in normalisation_techniques:\n",
    "    if normalisation == 'NoNormalisation':\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            cv_scores = cross_val_score(clf, X_b, y_b, cv=5, scoring='accuracy')\n",
    "            cv_scores_percentage = [round(score * 100, 2) for score in cv_scores]\n",
    "            print(f\"Cross-Validation without normalization and {clf_name} for FatigueB\")\n",
    "            print(\"CV Scores: \", cv_scores_percentage)\n",
    "            print(f\"Average CV Score:  {round(cv_scores.mean() * 100, 2)}%\")\n",
    "            print(\"--------------------------------------------\\n\")\n",
    "\n",
    "    else:\n",
    "        X_norm_b = normalisation.fit_transform(X_b)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            cv_scores = cross_val_score(clf, X_norm_b, y_b, cv=5, scoring='accuracy')\n",
    "            cv_scores_percentage = [round(score * 100, 2) for score in cv_scores]\n",
    "            print(f\"Cross-Validation with {normalisation} and {clf_name} for FatgueB\")\n",
    "            print(\"CV Scores: \", cv_scores_percentage)\n",
    "            print(f\"Average CV Score:  {round(cv_scores.mean() * 100, 2)}%\")\n",
    "            print(\"--------------------------------------------\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8fb05",
   "metadata": {},
   "source": [
    "| Scaler              | Model          | Average CV Score (%) | Min CV Score (%) | Max CV Score (%) |\n",
    "|---------------------|----------------|----------------------|------------------|------------------|\n",
    "| None                | SGDClassifier  | 76.49                | 58.0             | 92.0             |\n",
    "| None                | RandomForest   | 85.31                | 72.55            | 98.0             |\n",
    "| None                | SVM            | 79.35                | 62.75            | 94.0             |\n",
    "| None                | kNN            | 81.74                | 64.71            | 94.0             |\n",
    "| None                | DecisionTree   | 80.09                | 66.0             | 90.0             |\n",
    "| None                | XGBoost        | 90.45                | 74.0             | 100.0            |\n",
    "| StandardScaler      | SGDClassifier  | 81.27                | 60.0             | 94.0             |\n",
    "| StandardScaler      | RandomForest   | 85.31                | 70.0             | 98.0             |\n",
    "| StandardScaler      | SVM            | 86.53                | 68.63            | 96.0             |\n",
    "| StandardScaler      | kNN            | 83.73                | 66.67            | 92.0             |\n",
    "| StandardScaler      | DecisionTree   | 80.09                | 66.0             | 90.0             |\n",
    "| StandardScaler      | XGBoost        | 90.45                | 74.0             | 100.0            |\n",
    "| MinMaxScaler        | SGDClassifier  | 77.26                | 68.0             | 86.0             |\n",
    "| MinMaxScaler        | RandomForest   | 84.91                | 68.0             | 98.0             |\n",
    "| MinMaxScaler        | SVM            | 84.93                | 66.67            | 94.0             |\n",
    "| MinMaxScaler        | kNN            | 84.53                | 68.63            | 94.0             |\n",
    "| MinMaxScaler        | DecisionTree   | 80.09                | 66.0             | 90.0             |\n",
    "| MinMaxScaler        | XGBoost        | 90.45                | 74.0             | 100.0            |\n",
    "| Normalizer          | SGDClassifier  | 76.91                | 60.0             | 94.0             |\n",
    "| Normalizer          | RandomForest   | 86.91                | 72.55            | 100.0            |\n",
    "| Normalizer          | SVM            | 79.35                | 62.75            | 96.0             |\n",
    "| Normalizer          | kNN            | 81.34                | 64.71            | 96.0             |\n",
    "| Normalizer          | DecisionTree   | 83.28                | 68.0             | 94.0             |\n",
    "| Normalizer          | XGBoost        | 88.85                | 68.0             | 100.0            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93569ee0",
   "metadata": {},
   "source": [
    "### Results:\n",
    "-----------------------------------------\n",
    "\n",
    "**No Normalization:**\n",
    "- The best performing model in the non-normalized dataset is the XGBoost classifier with an accuracy of 90.45% (min: 74.40%, max: 100.0%).\n",
    "- The least performing model was the SGDClassifier with an accuracy score of 76.49% (min: 58.0%, max: 92.0%).\n",
    "\n",
    "**StandardScaler() Normalization:**\n",
    "- The best performing model with StandardScaler() normalization is the XGBoost classifier with an accuracy of 90.45 % (min: 74.0%, max: 100%).\n",
    "- The least performing model in this dataset with StandardScaler() normalization is the DecisionTree with an accuracy score of 80.09% (min: 66.0%, max: 90.0%).\n",
    "\n",
    "**MinMaxScaler() Normalization:**\n",
    "- The best performing model with MinMaxScaler() normalization is the XGBoost classifier with an accuracy of 90.45% (min: 74.0%, max: 100.0%).\n",
    "- The least performing model in this dataset with MinMaxScaler() normalization is the SGDClassifier with an accuracy score of 77.26% (min: 68.0%, max: 86.0%).\n",
    "\n",
    "**Normalizer() Normalization:**\n",
    "- The best performing model with Normalizer() normalization is the XGBoost classifier with an accuracy of 88.85% (min: 68.0%, max: 100.0%).\n",
    "- The least performing model in this dataset with Normalizer() normalization is the DecisionTree with an accuracy score of 76.91% (min: 60.0%, max: 94.0%).\n",
    "\n",
    "### Conclusion:\n",
    "-----------------------------\n",
    "\n",
    "Overall, the XGBoost saw as significant increase its accuracy score while working on `FatigueB` compared to `FatigueA`, with a peak of 90.45% accuracy recorded with No Normalization, StandardScaler(), MinMaxScaler() data, compared to 87.4% in FatigueA (Normalize() data only). However this is potentially due to severeal factors such as sample size, class distribution and variation in dataset. Outside this, accuracy where similar with only minor differences and as wiith teh `FatigueA` dataset, both DecisionTree and SDGClassifier had comparatively lower accuracy scores, struggling based on the type of Normalization technique used or lack there of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f2db0",
   "metadata": {},
   "source": [
    "### b. Changing Kernel Sizes From Taks 4 with Fatigue B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcee93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting data to 3d array\n",
    "X3d_b = X_b[:, np.newaxis, :]\n",
    "print(X3d_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b0de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_options = [5000, 10000, 20000, 50000]\n",
    "\n",
    "for kernel in kernel_options:\n",
    "    rocket = Rocket(num_kernels=kernel, random_state=42)\n",
    "    X3d_transformed = rocket.fit_transform(X3d)\n",
    "     \n",
    "\n",
    "    for classifer_name, classifier in classifiers.items():\n",
    "        \n",
    "        classifier.fit(X3d_transformed, y)\n",
    "\n",
    "        \n",
    "        cv_scores = cross_val_score(classifier, X3d_transformed, y, cv=5, scoring='accuracy')\n",
    "        cv_scores_percentage = [round(score * 100, 2) for score in cv_scores]\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Cross-Validation with {kernel} kernels and {classifer_name}\")\n",
    "        print(\"CV Scores: \", cv_scores_percentage)\n",
    "        print(f\"Average CV Score:  {round(cv_scores.mean() * 100, 2)}%\")\n",
    "        print(\"----------------------------------------------------\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61a80d",
   "metadata": {},
   "source": [
    "| Kernel Size | Model          | Average CV Score (%) | Min CV Score (%) | Max CV Score (%) |\n",
    "|-------------|----------------|----------------------|------------------|------------------|\n",
    "| 5000        | SGDClassifier  | 82.21                | 70.59            | 94.05            |\n",
    "| 5000        | RandomForest   | 93.13                | 84.71            | 97.62            |\n",
    "| 5000        | SVM            | 87.69                | 71.76            | 92.86            |\n",
    "| 5000        | kNN            | 87.93                | 69.41            | 96.43            |\n",
    "| 5000        | DecisionTree   | 82.45                | 72.94            | 91.67            |\n",
    "| 5000        | XGBoost        | 92.9                 | 82.35            | 96.43            |\n",
    "| 10000       | SGDClassifier  | 82.68                | 72.94            | 94.05            |\n",
    "| 10000       | RandomForest   | 92.18                | 82.35            | 97.62            |\n",
    "| 10000       | SVM            | 86.26                | 71.76            | 92.86            |\n",
    "| 10000       | kNN            | 87.69                | 68.24            | 97.62            |\n",
    "| 10000       | DecisionTree   | 85.98                | 79.76            | 90.48            |\n",
    "| 10000       | XGBoost        | 95.03                | 89.41            | 98.81            |\n",
    "| 20000       | SGDClassifier  | 81.06                | 52.94            | 96.43            |\n",
    "| 20000       | RandomForest   | 93.37                | 84.71            | 97.62            |\n",
    "| 20000       | SVM            | 86.02                | 71.76            | 92.86            |\n",
    "| 20000       | kNN            | 87.93                | 68.24            | 96.43            |\n",
    "| 20000       | DecisionTree   | 84.59                | 70.59            | 90.48            |\n",
    "| 20000       | XGBoost        | 95.26                | 90.59            | 100.0            |\n",
    "| 50000       | SGDClassifier  | 91.7                 | 84.52            | 96.43            |\n",
    "| 50000       | RandomForest   | 93.37                | 83.53            | 97.62            |\n",
    "| 50000       | SVM            | 86.97                | 71.76            | 92.86            |\n",
    "| 50000       | kNN            | 88.17                | 69.41            | 97.62            |\n",
    "| 50000       | DecisionTree   | 82.91                | 76.47            | 89.29            |\n",
    "| 50000       | XGBoost        | 92.19                | 80.0             | 100.0            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b91018",
   "metadata": {},
   "source": [
    "### Results\n",
    "--------------------------------------------\n",
    "Best Performing Models By Kernel Size:\n",
    "\n",
    "* ROCKET = 5000 Kernels:\n",
    "\n",
    "    * Best Performing Model: The RandomForest was the best performing model with an average accuracy of 93.13% (Min: 84.71%, Max: 97.62%).\n",
    "    * Worst Performing Model: The SDGClassifier was the least performing model with an average accuracy score of 82.21% (Min: 70.95%, Max: 94.05%).\n",
    "\n",
    "* ROCKET = 10000 Kernels:\n",
    "\n",
    "    * Best Performing Model: The XGBoost was the best performing model with an average accuracy of 95.03% (Min: 89.41%, Max: 96.43%).\n",
    "    * Worst Performing Model: The SDGClassifier was the least performing model with an average accuracy score of 82.68% (Min: 72.62%, Max: 94.05%).\n",
    "\n",
    "* ROCKET = 20000 Kernels:\n",
    "\n",
    "    * Best Performing Model: The XGBoost was the best performing model with an average accuracy of 95.26% (Min: 90.59%, Max: 100.0%).\n",
    "    * Worst Performing Model: The SDGClassifier was the least performing model with an average accuracy score of 81.06% (Min: 52.94%, Max: 89.29%).\n",
    "\n",
    "* ROCKET = 50000 Kernels:\n",
    "\n",
    "    * Best Performing Model: The XGBoost was the best performing model with an average accuracy of 93.37% (Min: 83.53%, Max: 97.62%).\n",
    "    * Worst Performing Model: The DecisionTree was the least performing model with an average accuracy score of 82.91% (Min: 76.67%, Max: 89.29%).\n",
    "\n",
    "    \n",
    "Conclusion\n",
    "In summary, this analysis revealed important insights about the performance of machine learning models under different kernel sizes in the context of ROCKET. Key findings include:\n",
    "\n",
    "Model Robustness: Both XGBoost and RandomForest consistently outperformed other models across various kernel sizes. These models demonstrated their robustness in handling kernel complexities and scalability.\n",
    "\n",
    "Kernel Size Impact: The choice of kernel size significantly affected model performance. In particular, the XGBoost model excelled when the kernel size was 10000 and 20000, with average accuracies of 95.03% and 95.26%, respectively.\n",
    "\n",
    "Sensitivity to Kernel Size: On the other hand, the SDGClassifier struggled to deliver competitive accuracy scores across all kernel sizes, highlighting its sensitivity to this parameter.\n",
    "\n",
    "Moderate Variability: SVM and kNN exhibited moderate performance variability across different kernel sizes but did not reach the consistent high performance levels of XGBoost or RandomForest.\n",
    "\n",
    "Overall, the results emphasize the importance of selecting the right model and kernel size for specific tasks involving ROCKET features. The XGBoost and RandomForest models stand out as strong choices for scalability and performance, while the DecisionTree and SDGClassifier models may require additional optimization or consideration for specific use cases. Furthermore significant increas in kernel size does not lead to significant increase in any of the models, suggesting there is a point where it's effect plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c42943",
   "metadata": {},
   "source": [
    "#### Comparing Results FatigueA and Fatigue B\n",
    "----------------------------------------------\n",
    "Overall, the results of both data and only minor differences which could be attributable to differences or variations in the data contained in both datasets. Both dataset continuously showed RandomForest and XGBoost in particular,  as the best performing models across kernel size and normalisation techniques. Also in both we can notice after a certain point, adding new kernels do not significantly improves the models accuracy, meaning it might plateau at a certain number and further increase are redundant. In summary the solution and results of FatigueB confirms most of our observations in FatigueA, where resutls vary, as discussed earlier is more likely due to dataset-specific differences rather than anything else. we can also observe how different changes in the subset of data can greatly affect the accuracy of a model as seen in the CV scores, giving credence to this theoory.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
